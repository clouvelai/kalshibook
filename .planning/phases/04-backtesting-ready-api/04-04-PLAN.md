---
phase: 04-backtesting-ready-api
plan: 04
type: execute
wave: 3
depends_on: ["04-01", "04-03"]
files_modified:
  - src/api/routes/candles.py
  - src/api/routes/events.py
  - src/api/services/candles.py
  - src/api/main.py
  - static/llms.txt
  - static/llms-full.txt
autonomous: true

must_haves:
  truths:
    - "GET /candles/{ticker} returns OHLC candlestick data at 1-minute, 1-hour, and 1-day intervals"
    - "Candles are computed from raw trade data using SQL aggregation (not proxied from Kalshi)"
    - "GET /events lists events with market counts"
    - "GET /events/{event_ticker} returns event detail with nested markets"
    - "All new endpoints documented in llms.txt and llms-full.txt"
    - "Empty candle buckets (no trades) produce no rows (documented behavior)"
  artifacts:
    - path: "src/api/routes/candles.py"
      provides: "GET /candles/{ticker} endpoint"
      contains: "require_credits"
    - path: "src/api/services/candles.py"
      provides: "SQL candlestick computation"
      contains: "date_trunc"
    - path: "src/api/routes/events.py"
      provides: "GET /events and GET /events/{event_ticker} endpoints"
      contains: "require_credits"
    - path: "static/llms.txt"
      provides: "Updated AI agent discovery with new endpoints"
      contains: "trades"
    - path: "static/llms-full.txt"
      provides: "Comprehensive API reference with all Phase 4 endpoints"
      contains: "candles"
  key_links:
    - from: "src/api/routes/candles.py"
      to: "src/api/services/candles.py"
      via: "route calls service for SQL aggregation"
      pattern: "get_candles"
    - from: "src/api/services/candles.py"
      to: "trades table"
      via: "SQL date_trunc aggregation query"
      pattern: "date_trunc"
    - from: "src/api/routes/events.py"
      to: "events and markets tables"
      via: "JOIN for nested markets in event detail"
      pattern: "event_ticker"
---

<objective>
Create candlestick and event hierarchy API endpoints, then update AI agent discovery files with all Phase 4 endpoints.

Purpose: Candles enable strategy-level backtesting (directional/momentum) without needing full L2 replay. Event hierarchy enables multi-market analysis (e.g., all strike prices for an event). The llms.txt update ensures AI agents discover the complete API surface.

Output: Two new route files, one service file, updated llms.txt files.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-backtesting-ready-api/04-01-SUMMARY.md
@.planning/phases/04-backtesting-ready-api/04-03-SUMMARY.md
@src/api/routes/markets.py
@src/api/routes/deltas.py
@src/api/deps.py
@src/api/models.py
@src/api/main.py
@static/llms.txt
@static/llms-full.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create candlestick service and endpoint</name>
  <files>
    src/api/services/candles.py
    src/api/routes/candles.py
  </files>
  <action>
**src/api/services/candles.py** -- NEW FILE. SQL-based candlestick computation:

Define the candle query as a module-level constant:
```python
CANDLE_QUERY = """
SELECT
    date_trunc($4, ts AT TIME ZONE 'UTC') AS bucket,
    market_ticker,
    (array_agg(yes_price ORDER BY ts ASC))[1] AS open,
    MAX(yes_price) AS high,
    MIN(yes_price) AS low,
    (array_agg(yes_price ORDER BY ts DESC))[1] AS close,
    SUM(count) AS volume,
    COUNT(*) AS trade_count
FROM trades
WHERE market_ticker = $1
  AND ts >= $2
  AND ts < $3
GROUP BY bucket, market_ticker
ORDER BY bucket ASC
"""
```

Create `async def get_candles(pool, market_ticker, start_time, end_time, interval) -> list[dict]`:
- Validate interval: must be one of `'minute'`, `'hour'`, `'day'`. Raise ValidationError if not.
- Execute CANDLE_QUERY with params: market_ticker, start_time, end_time, interval
- Return list of dicts with keys: bucket, market_ticker, open, high, low, close, volume, trade_count
- Empty result is valid (no trades in range)

Define `VALID_INTERVALS = {'1m': 'minute', '1h': 'hour', '1d': 'day'}` mapping from user-facing interval strings to Postgres date_trunc arguments.

**src/api/routes/candles.py** -- NEW FILE:
- `router = APIRouter(tags=["Candles"])`
- `GET /candles/{ticker}` endpoint:
  - Signature: `async def get_candles_endpoint(request: Request, ticker: str, start_time: datetime = Query(...), end_time: datetime = Query(...), interval: str = Query(default="1h", description="Candle interval: 1m, 1h, or 1d"), key: dict = Depends(require_credits(3)), pool: asyncpg.Pool = Depends(get_db_pool))`
  - Credit cost: 3 (aggregation query, moderate compute)
  - Validate `interval` is in VALID_INTERVALS, raise ValidationError if not
  - Call `candles_service.get_candles(pool, ticker, start_time, end_time, VALID_INTERVALS[interval])`
  - Map results to `CandleRecord` models (bucket as ISO 8601)
  - Return `CandlesResponse` with data, request_id, response_time
  - Note in docstring: "Buckets with no trades produce no rows. Consumers typically forward-fill the previous close price for empty intervals."
  </action>
  <verify>
1. `python -c "from src.api.services.candles import get_candles, VALID_INTERVALS; print('OK')"` -- imports work
2. `python -c "from src.api.routes.candles import router; print('OK')"` -- imports work
  </verify>
  <done>GET /candles/{ticker} endpoint exists with 1m/1h/1d interval support, computing OHLC + volume from raw trade data via SQL aggregation. Credit cost is 3. Empty buckets documented as expected behavior.</done>
</task>

<task type="auto">
  <name>Task 2: Create events endpoint, register all routers, and update llms.txt files</name>
  <files>
    src/api/routes/events.py
    src/api/main.py
    static/llms.txt
    static/llms-full.txt
  </files>
  <action>
**src/api/routes/events.py** -- NEW FILE:
- `router = APIRouter(tags=["Events"])`
- `GET /events` endpoint:
  - Signature: `async def list_events(request: Request, key: dict = Depends(require_credits(1)), pool: asyncpg.Pool = Depends(get_db_pool), category: str | None = None, series_ticker: str | None = None, status: str | None = None)`
  - Credit cost: 1 (list query)
  - Query events table with optional filtering by category, series_ticker, status. Include a subquery for market_count: `(SELECT COUNT(*) FROM markets WHERE event_ticker = e.event_ticker)`.
  - Map to `EventSummary` models
  - Return `EventsResponse`

- `GET /events/{event_ticker}` endpoint:
  - Signature: `async def get_event_detail(request: Request, event_ticker: str, key: dict = Depends(require_credits(1)), pool: asyncpg.Pool = Depends(get_db_pool))`
  - Credit cost: 1
  - Fetch event from events table. If not found, raise 404 error (add `EventNotFoundError` to errors.py following `MarketNotFoundError` pattern).
  - Fetch all markets for this event: `SELECT ticker, title, event_ticker, status, category FROM markets WHERE event_ticker = $1 ORDER BY ticker`
  - Map to `EventDetail` model with nested `markets: list[MarketSummary]`
  - Return `EventDetailResponse`

**src/api/main.py** -- Update:
1. Add imports: `from src.api.routes import candles, events`
2. Add OpenAPI tags:
```python
{
    "name": "Candles",
    "description": "OHLC candlestick data at 1-minute, 1-hour, and 1-day intervals.",
},
{
    "name": "Events",
    "description": "Navigate Series > Event > Market hierarchy.",
},
```
3. Register routers: `app.include_router(candles.router)` and `app.include_router(events.router)`

**static/llms.txt** -- Update with new endpoints section. Read the existing file first, then append a section covering:
- POST /trades -- trade history with pagination (2 credits)
- GET /settlements, GET /settlements/{ticker} -- settlement data (1 credit each)
- GET /candles/{ticker} -- OHLC candlestick data (3 credits, intervals: 1m/1h/1d)
- GET /events, GET /events/{event_ticker} -- event hierarchy (1 credit each)

**static/llms-full.txt** -- Update with comprehensive documentation for all new endpoints. Read existing file first, then append sections covering:
- **Trades**: POST /trades request body (market_ticker, start_time, end_time, cursor, limit), response format, cursor pagination, credit cost
- **Settlements**: GET /settlements (optional filters: event_ticker, result), GET /settlements/{ticker}, response format, credit cost
- **Candles**: GET /candles/{ticker} query params (start_time, end_time, interval), OHLC fields (open, high, low, close from yes_price), volume, trade_count, empty bucket behavior, credit cost
- **Events**: GET /events (optional filters: category, series_ticker, status), GET /events/{event_ticker} with nested markets, credit cost
- **Updated endpoint summary table** showing all endpoints with credit costs
- **Updated backtesting workflow** section describing how to use the new endpoints together

Follow the existing llms-full.txt style exactly (markdown formatting, section headers, code examples).
  </action>
  <verify>
1. `python -c "from src.api.routes.events import router; from src.api.routes.candles import router; print('OK')"` -- imports
2. Start API: `uv run uvicorn src.api.main:app --host 0.0.0.0 --port 8000 &` then:
   - `curl -s http://localhost:8000/openapi.json | python -c "import sys,json; d=json.load(sys.stdin); print([t['name'] for t in d['tags']])"` shows all 4 new tags (Trades, Settlements, Candles, Events)
   - `curl -s http://localhost:8000/llms.txt` mentions trades, settlements, candles, events
   - `curl -s http://localhost:8000/llms-full.txt | wc -l` is substantially longer than before
3. Verify all existing endpoints still work: `curl -s http://localhost:8000/health` returns 200
  </verify>
  <done>GET /events and GET /events/{event_ticker} endpoints exist with hierarchy navigation. GET /candles/{ticker} registered. All 4 new endpoint groups appear in OpenAPI spec. llms.txt and llms-full.txt updated with complete documentation for all Phase 4 endpoints.</done>
</task>

</tasks>

<verification>
1. GET /candles/{ticker} accepts start_time, end_time, interval params
2. Candle computation uses SQL date_trunc, not Kalshi REST proxy
3. GET /events lists events with market counts
4. GET /events/{event_ticker} returns nested markets
5. All 4 new endpoint groups appear in OpenAPI spec
6. llms.txt mentions all new endpoints
7. llms-full.txt has comprehensive docs for all new endpoints
8. Credit costs: candles=3, events=1, trades=2, settlements=1
9. No existing endpoints broken
</verification>

<success_criteria>
- All 4 new endpoint groups (trades, settlements, candles, events) appear in /openapi.json
- Candles computed from trades table via SQL aggregation
- Events endpoint shows nested markets for an event
- llms.txt and llms-full.txt fully document the complete API surface
- All endpoints use require_credits with costs matching research recommendations
- API server starts and serves all endpoints without errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-backtesting-ready-api/04-04-SUMMARY.md`
</output>
